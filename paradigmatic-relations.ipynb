{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing with Neo4j: Mining Paradigmatic Relations\n",
    "\n",
    "This IPython notebook is the companion for this [blog post](http://lyonwj.com/2015/06/16/nlp-with-neo4j/) about getting started with Natural Language Processing using Neo4j. This notebook mostly covers data insertion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "import re, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# connect to Neo4j instance using py2neo - default running locally\n",
    "graphdb = Graph('http://neo4j:neo4j@localhost:7474/db/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define some parameterized Cypher queries\n",
    "\n",
    "# For data insertion\n",
    "INSERT_QUERY = '''\n",
    "    FOREACH (t IN {wordPairs} | \n",
    "        MERGE (w0:Word {word: t[0]})\n",
    "        MERGE (w1:Word {word: t[1]})\n",
    "        CREATE (w0)-[:NEXT_WORD]->(w1)\n",
    "        )\n",
    "'''\n",
    "\n",
    "# get the set of words that appear to the left of a specified word in the text corpus\n",
    "LEFT1_QUERY = '''\n",
    "    MATCH (s:Word {word: {word}})\n",
    "    MATCH (w:Word)-[:NEXT_WORD]->(s)\n",
    "    RETURN w.word as word\n",
    "'''\n",
    "\n",
    "# get the set of words that appear to the right of a specified word in the text corpus\n",
    "RIGHT1_QUERY = '''\n",
    "    MATCH (s:Word {word: {word}})\n",
    "    MATCH (w:Word)<-[:NEXT_WORD]-(s)\n",
    "    RETURN w.word as word\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Data insertion\n",
    "#### Normalizing the data (lowercase, remove punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a regular expression to remove punctuation\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "exclude = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert a sentence string into a list of lists of adjacent word pairs\n",
    "# arrifySentence(\"Hi there, Bob!) = [[\"hi\", \"there\"], [\"there\", \"bob\"]]\n",
    "def arrifySentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.strip()\n",
    "    sentence = regex.sub('', sentence)\n",
    "    wordArray = sentence.split()\n",
    "    tupleList = []\n",
    "    for i, word in enumerate(wordArray):\n",
    "        if i+1 == len(wordArray):\n",
    "            break\n",
    "        tupleList.append([word, wordArray[i+1]])\n",
    "    return tupleList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Load the data\n",
    "The text corpus used here is the [CEEAUS](http://earthlab.uoi.gr/theste/index.php/theste/article/viewFile/55/37) [corpus](http://language.sakura.ne.jp/s/eng.html) which is distributed with the [MeTA NLP library](https://meta-toolkit.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadFile():\n",
    "    tx = graphdb.cypher.begin()\n",
    "    with open('data/ceeaus.dat', encoding='ISO-8859-1') as f:\n",
    "        count = 0\n",
    "        for l in f:\n",
    "            params = {'wordPairs': arrifySentence(l)}\n",
    "            tx.append(INSERT_QUERY, params)\n",
    "            tx.process()\n",
    "            count += 1\n",
    "            if count > 300:\n",
    "                tx.commit()\n",
    "                tx = graphdb.cypher.begin()\n",
    "                count = 0\n",
    "    f.close()\n",
    "    tx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loadFile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Paradigmatic relations\n",
    "\n",
    "We first define two functions (`left1` and `right1`) to allow us to represent each word by its context (as a set of words). We then define a function to compute the [Jaccard similarity coefficient](https://en.wikipedia.org/wiki/Jaccard_index) given two arbitrary sets which will allow us to compute a measure of Paradigmatic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# return a set of all words that appear to the left of `word`\n",
    "def left1(word):\n",
    "    params = {\n",
    "        'word': word.lower()\n",
    "    }\n",
    "    tx = graphdb.cypher.begin()\n",
    "    tx.append(LEFT1_QUERY, params)\n",
    "    results = tx.commit()\n",
    "    words = []\n",
    "    for result in results:\n",
    "        for line in result:\n",
    "            words.append(line.word)\n",
    "    return set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return a set of all words that appear to the right of `word`\n",
    "def right1(word):\n",
    "    params = {\n",
    "        'word': word.lower()\n",
    "    }\n",
    "    tx = graphdb.cypher.begin()\n",
    "    tx.append(RIGHT1_QUERY, params)\n",
    "    results = tx.commit()\n",
    "    words = []\n",
    "    for result in results:\n",
    "        for line in result:\n",
    "            words.append(line.word)\n",
    "    return set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute Jaccard coefficient\n",
    "def jaccard(a,b):\n",
    "    intSize = len(a.intersection(b))\n",
    "    unionSize = len(a.union(b))\n",
    "    return intSize / unionSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we define paradigmatic similarity as the average of the Jaccard coefficents of the `left1` and `right1` sets\n",
    "def paradigSimilarity(w1, w2):\n",
    "    return (jaccard(left1(w1), left1(w2)) + jaccard(right1(w1), right1(w2))) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20997973657548125"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the measure of paradigmatic similarity between \"school\" and \"university\" in the corpus?\n",
    "paradigSimilarity(\"school\", \"university\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
